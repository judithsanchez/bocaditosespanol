import {tokenizeSentences} from '../tokenizeSentences';

describe('tokenizeSentences', () => {
	test('returns null for an empty string', () => {
		const sentence = '';
		const result = tokenizeSentences(sentence);
		expect(result).toBeNull();
	});

	test('tokenizes a simple sentence correctly', () => {
		const sentence = 'Hello, world!';
		const expected = {
			originalSentence: sentence,
			tokens: ['Hello', ',', 'world', '!'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	test('tokenizes a sentence with multiple punctuation marks correctly', () => {
		const sentence = 'Hello, world! How are you?';
		const expected = {
			originalSentence: sentence,
			tokens: ['Hello', ',', 'world', '!', 'How', 'are', 'you', '?'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	test('tokenizes a sentence with emojis correctly', () => {
		const sentence = 'Hello ğŸ‘‹ğŸ», world! ğŸ˜€';
		const expected = {
			originalSentence: sentence,
			tokens: ['Hello', 'ğŸ‘‹ğŸ»', ',', 'world', '!', 'ğŸ˜€'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	test('tokenizes a sentence with ellipsis correctly', () => {
		const sentence = 'Hello... world!';
		const expected = {
			originalSentence: sentence,
			tokens: ['Hello', '...', 'world', '!'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	test('handles emojis with skin tone modifiers', () => {
		const sentence = 'ğŸ‘‹ğŸ» ğŸ‘‹ğŸ¼ ğŸ‘‹ğŸ½ ğŸ‘‹ğŸ¾ ğŸ‘‹ğŸ¿';
		const expected = {
			originalSentence: sentence,
			tokens: ['ğŸ‘‹ğŸ»', 'ğŸ‘‹ğŸ¼', 'ğŸ‘‹ğŸ½', 'ğŸ‘‹ğŸ¾', 'ğŸ‘‹ğŸ¿'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	test('handles emojis with gender modifiers', () => {
		const sentence = 'ğŸ‘¨â€ğŸ’» ğŸ‘©â€ğŸ’»';
		const expected = {
			originalSentence: sentence,
			tokens: ['ğŸ‘¨â€ğŸ’»', 'ğŸ‘©â€ğŸ’»'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	// test('handles emojis with multiple modifiers', () => {
	// 	expect(tokenizeSentences('ğŸ‘©ğŸ½â€ğŸ’» ğŸ‘¨ğŸ¼â€ğŸ³')).toBe('ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ³');
	// });

	// test('handles emojis with and without modifiers', () => {
	// 	expect(tokenizeSentences('ğŸ‘‹ğŸ» ğŸ‘‹ ğŸ‘©ğŸ½â€ğŸ’» ğŸ‘¨â€ğŸ³')).toBe('ğŸ‘‹ğŸ‘‹ğŸ‘©â€ğŸ’»ğŸ‘¨â€ğŸ³');
	// });
	test('handles emojis with multiple modifiers', () => {
		const sentence = 'ğŸ‘©ğŸ½â€ğŸ’» ğŸ‘¨ğŸ¼â€ğŸ³';
		const expected = {
			originalSentence: sentence,
			tokens: ['ğŸ‘©ğŸ½â€ğŸ’»', 'ğŸ‘¨ğŸ¼â€ğŸ³'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});

	test('handles emojis with and without modifiers', () => {
		const sentence = 'ğŸ‘‹ğŸ» ğŸ‘‹ ğŸ‘©ğŸ½â€ğŸ’» ğŸ‘¨â€ğŸ³';
		const expected = {
			originalSentence: sentence,
			tokens: ['ğŸ‘‹ğŸ»', 'ğŸ‘‹', 'ğŸ‘©ğŸ½â€ğŸ’»', 'ğŸ‘¨â€ğŸ³'],
		};
		const result = tokenizeSentences(sentence);
		expect(result).toEqual(expected);
	});
});
